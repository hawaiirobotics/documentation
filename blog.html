<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <title>hawaii robotics</title>
    <link rel="stylesheet" href="main.css">
    <link rel="icon" href="favicon.ico">
</head>
<body>
    <header>
        <nav>
            <h1 class="title"><a href="index.html">Hawaii Robotics</a></h1>
            <ul class="nav-links">
                <li><a href="blog.html">Blog</a></li>
                <li><a href="team.html">Team</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <div class="wrapper">
            <h2 style="margin-top: 16px;">Blog</h2>
            <hr>
            <ul class="blog-list">
                <!--Post 6-->
                <li>
                    <h2 class="blog-link">FDP</h2>
                    <h3 class="blog-date">Nov 27, 2023</h3>
                    <div class="blog-post">
                        <button type="button" class="blog-close">
                            <img src="./assets/close button.svg" alt="Close" style="width: 2em; height: auto; margin: 0;">
                        </button>
                        <div class="blog-title-box">
                            <h2 class="blog-title">FDP Slides</h2>
                            <p class="blog-subtitle">Final Design Presentation</p>
                        </div>
                        <div class="blog-content">
                            <img style="margin-top: 16px;" src="./assets/FDP/FDP_Slide (1).PNG">
                            <img src="./assets/FDP/FDP_Slide (2).PNG">
                            <img src="./assets/FDP/FDP_Slide (3).PNG">
                            <img src="./assets/FDP/FDP_Slide (4).PNG">
                            <img src="./assets/FDP/FDP_Slide (5).PNG">
                            <img src="./assets/FDP/FDP_Slide (6).PNG">
                            <img src="./assets/FDP/FDP_Slide (7).PNG">
                            <img src="./assets/FDP/FDP_Slide (8).PNG">
                            <img src="./assets/FDP/FDP_Slide (9).PNG">
                            <img src="./assets/FDP/FDP_Slide (10).PNG">
                            <img src="./assets/FDP/FDP_Slide (11).PNG">
                            <img src="./assets/FDP/FDP_Slide (12).PNG">
                            <img src="./assets/FDP/FDP_Slide (13).PNG">
                            <img src="./assets/FDP/FDP_Slide (14).PNG">
                            <img src="./assets/FDP/FDP_Slide (15).PNG">
                            <img src="./assets/FDP/FDP_Slide (16).PNG">
                            <img src="./assets/FDP/FDP_Slide (17).PNG">
                            <img src="./assets/FDP/FDP_Slide (18).PNG">
                            <img src="./assets/FDP/FDP_Slide (19).PNG">
                            <img src="./assets/FDP/FDP_Slide (20).PNG">
                            <img src="./assets/FDP/FDP_Slide (21).PNG">
                            <img src="./assets/FDP/FDP_Slide (22).PNG">
                            <img src="./assets/FDP/FDP_Slide (23).PNG">
                            <img src="./assets/FDP/FDP_Slide (24).PNG">
                            <img src="./assets/FDP/FDP_Slide (25).PNG">
                            <img src="./assets/FDP/FDP_Slide (26).PNG">
                            <img src="./assets/FDP/FDP_Slide (27).PNG">
                        </div>
                    </div>
                </li>
                <!--Post 6-->
                <li>
                    <h2 class="blog-link">Mechanical Development</h2>
                    <h3 class="blog-date">Nov 26, 2023</h3>
                    <div class="blog-post">
                        <button type="button" class="blog-close">
                            <img src="./assets/close button.svg" alt="Close" style="width: 2em; height: auto; margin: 0;">
                        </button>
                        <div class="blog-title-box">
                            <h2 class="blog-title">Mechanical Development</h2>
                            <p class="blog-subtitle">Progress on the Student Arm, Teacher Arm, and Enclosure</p>
                        </div>
                        <div class="blog-content">
                            <div class="text-box">
                                <h2>Overview</h2>
                            </div>
                            <div class="text-box">
                                <p>After the design concept selection, the preliminary design phase can begin. A lot of inspiration was drawn from the ALOHA paper [1]. The student arms should be enclosed in a rigid structure, with a consistent and well lit environment. The rigid enclosure helps keep people safe from the robots, and also provides a convenient way to mount the arms rigidly and at a fixed distance from each other. A consistent environment and good lighting is important for the model to be able to stay consistent. For example if we collected data at home, trained a model, and then brought the system to symposium and the cameras could see different things in the background the system ma behave differently. For this reason we want to use all opaque paneling with all lighting contained in the enclosure. An effort will be made to try to reduce the complexity of the environment, keeping it as simple as possible.</p>
                            </div>
                            <div class="text-box">
                                <p>As outlined in our design concept selection, the 2 teacher arm teleoperation system won out over the exoskeleton and VR/AR approach. In order to communicate more clearly how we imagine this system working, please see the image below. Essentially the teacher arms are a version of the student arms that are kinematically equivalent, but much lighter and simpler.</p>
                            </div>
                            <img src="./assets/mechanical1.png">
                            <div class="text-box">
                                <p>As can be seen in the picture above the teacher arms have a 4 bar mechanism and springs. This helps to make the system weightless, reducing the load on the operator, which is a common complaint from ALOHA users [1]. The design of this system will be broken down in another section of this report. We envision the teacher arms existing as a separate piece of equipment that can be used on other student arm systems, and almost acts like a “mouse” or “remote” for them. We also have the ambitious goal of using the teacher arms on their own for training in simulation, and are partnered with a researcher to test this method of training out.</p>
                            </div>
                            <div class="text-box">
                                <h2>Student Arms</h2>
                            </div>
                            <div class="text-box">
                                <p>In order to engineer the best arms possible, and optimize on cost, it was important to start from first principles. What are the actual hard requirements we need to meet? Why do they exist? Why do we exist? Once these questions have been pondered, the optimal arm design can be found from these requirements. To do this we start by listing the requirements in our arm calculator tool, and then input some assumed values (which get iterated throughout the design process) such as linkage length and masses. In the case of the table below the requirements are marked in blue, the unknown values in yellow, and the assumed values in orange. As we iterate, the assumed values turn into more “design inputs” that can technically be changed but are effectively locked for all intents and purposes. Once the rough geometry of the arm is sorted out, and the input speeds and loads are set we can start to calculate joint torques and the loads across the arm, this is shown in the table below.</p>
                            </div>
                            <img src="./assets/mechanical2.png">
                            <div class="text-box">
                                <p>Our motor selection tool can be seen in the table below. The motor selection also feeds back the weight of each motor into the torque requirements for the arm. This allows us to iterate on motors and see the changes to the joint torques live. The most important part of this calculation is taking into consideration motor nonlinearities, since motors don’t just have a constant speed and torque, they are both values that change in different operating regions of the motor – which this calculation accounts for. It also provides a visually intuitive way of viewing the factor of safety, allowing for quick iteration through 50+ different options for motors. Finally, the cost is considered on the far right column, this is very important given how expensive these servos are. This tool is one of the ways we were able to drastically reduce the cost of this arm compared to the ALOHA system which costs roughly twice as much [1].</p>
                            </div>
                            <img src="./assets/mechanical3.png">
                            <div class="text-box">
                                <p>Finally the joint torques are used again to calculate how stiff the assembly has to be. This requirement can be broken down into how stiff each joint needs to be, by assuming that each part in series has similar stiffness. To do this we take the maximum deflection requirement, and the maximum payload, and get a linear stiffness. This linear stiffness can be converted to an angular stiffness using the reach of the arm. Then each linkage in the series elastic stack up is designed to meet a fraction of this deflection, such that the total deflection of the system is within spec. This calculation as well as one of many FEA simulations is shown in the image below.</p>
                            </div>
                            <img src="./assets/mechanical4.png">
                            <div class="text-box">
                                <p>For the structural design of the arm, we found it was cost optimal to reduce the weight of the linkages, especially in the forearm and gripper portion of the arm since the weight contributes a higher moment. To reduce weight as much as possible, we decided to use carbon fiber tube sections which are extraordinarily stiff, strong, and very effective at reducing EMI in cables routed through the tubes. To connect the carbon fiber, three technologies will be utilized. The first it SLM Metal 3D printing, which allows us to print generatively designed mass optimized parts out of exotic materials like stainless steel. The linkage on J2 (the shoulder) and J3 (the elbow) are manufactured using this technology. In joints with tricky geometry, CNC machined 6061 aluminum will be used, and in simpler joints, bent aluminum sheet metal will be used. The final design of the student arm can be seen in the image below.</p>
                            </div>
                            <img src="./assets/mechanical5.png">
                            <div class="text-box">
                                <h2>Teacher Arms</h2>
                            </div>
                            <div class="text-box">
                                <p>The design of the teacher arms is one of the biggest levers we have to make this project successful. This is because other attempts at making them have significant downsides, mainly due to ease of use and cost. For example, in both GELLO [4] and ALOHA [1] user fatigue is mentioned. And from users who we have surveyed, they mention it being very painful on the fingers to operate for long periods of time – typical for gathering training data. This is mostly because both GELLO and ALOHA use servos as sensors, they do not power them on at all, they just read the position from them. From these points we can generate a list of improvements to keep in mind during the design phase:</p>
                            </div>
                            <div class="text-box">
                                <p>1.   Easy gripper activation, comfortable to hold<br>
                                    2.   Virtually weightless to move around<br>
                                    3.   Low cost</p>
                            </div>
                            <div class="text-box">
                                <p>Starting with the gripper improvements, both GELLO and ALOHA have similar designs. They both force the user to backdrive a powerful servo in order to read the position, in the ALOHA case there is also a ton of friction and the movement is non-linear. Please see the picture below for a detailed view of the ALOHA teacher arm end effector.</p>
                            </div>
                            <img src="./assets/mechanical6.png">
                            <div class="text-box">
                                <p>Next is the weightlessness, and since ALOHA and GELLO both use servos on all joints, the arm is partially held by friction. But this is also a bad thing since the user has to backdrive all these servos to move. Instead we invision just using encoders on all the joints, and bushings to reduce the friction. This will lead to seamless, low force movements. We also plan on using passive springs to reduce the weight of the system felt by the user, and for this we look to the common desk lamp shown in the picture below.</p>
                            </div>
                            <img src="./assets/mechanical7.png">
                            <div class="text-box">
                                <p>Placing the springs is quite a difficult problem. A four bar linkage, like the one in the picture above can be used to keep the second linkage spring in a fixed position, but choosing the mounting positions of these springs is hard. Because of this, we have developed a tool that performs inverse kinematics on 5000 normally distributed points around the main working envelope, and converges on the optimal spring, and optimal mounting locations to make the system completely weightless. A screenshot of this tool while solving this optimization problem is shown in the image below.</p>
                            </div>
                            <img src="./assets/mechanical8.png">
                            <div class="text-box">
                                <p>Our current progress on the teacher arm is a prototype to validate the kinematics and the encoder strategy. Pictures of the modular bending and twisting joints can be seen below and a demonstration of the arm can be seen under that. It’s currently not sprung and heavier than we would like it, so a new iteration is required. The next version will incorporate the four bar linkage, springs, and smaller encoder PCBs. The structure will also shift to carbon fiber to make it as light as possible.</p>
                            </div>
                            <img src="./assets/mechanical9.png">
                            <img src="./assets/mechanical10.png">
                            <div class="text-box">
                                <h2>Enclosure</h2>
                            </div>
                            <div class="text-box">
                                <p>The enclosure houses the student arms and serves a couple purposes. The first is rigid mounting for both arms holding them at precise locations from one another with a frame that is strong enough to minimize movement of the base. Especially as the system is semi portable this is an important aspect of the design. The enclosure also allows for mounting points of electronics and harnesses which need to connect to the teacher arms and student arms. The enclosure serves a safety purpose preventing injury while the robot is running. It also acts to provide a consistent background for the vision system and consistent lighting. Since it is sealed from external lighting and has its own internal lighting, we have control over how the robot and work area are lit so we can maximize repeatability with minimal scenery changes.</p>
                            </div>
                            <div class="text-box">
                                <p>The enclosure itself is constructed of 4040 aluminum extrusion with custom gusset plates and corner joints to hold the pieces together. For the front and back there are sliding doors which use rollers that run in the extrusion. The sliding doors themselves are also composed of 4040 aluminum extrusion. The cameras that aren’t on the arms which are the top and front facing cameras mount directly to the extrusion and wires along with light strips are routed along the extrusion. The panels are composed of high-density fiberboard with a white acrylic coating on one side. This matte white coating gives a consistent background and helps give good contrast against the arms and objects being handled.</p>
                            </div>
                            <div class="text-box">
                                <p>The fiberboard on the bottom of the enclosure has small semi-circular cut outs for wire routing in the corners which can be run through the bottom of the enclosure to the Honolulu board. The Honolulu board is mounted on the left side of the enclosure. A registered jack connector is used to connect to the student arms and a small electrical box with the high voltage to low voltage convertor is also located on this side of the enclosure. The E-Stop is affixed to the electrical box to cut power to the system which would include any attached student arm system through the registered jack connection. When moving the system around it can be placed on a table with caster wheels or easily carried around. Our enclosure is designed in such a way where it is easy to manufacture, assemble and modify. Using extrusion, we achieve these goals as extrusion is simple to source and is analogous to LEGO for mechanical engineers.</p>
                            </div>
                            <div class="text-box">
                                <p>Another major part of the enclosure is the lighting control through integrated light strips. The light strips we have selected utilize WS2812 individually addressable LEDs. These LEDs allow for the control of color and brightness across each individual LED, and we target about 100 of them throughout the enclosure. This will allow us to adjust where the light is coming from and the color which is used. This can help us achieve the contrast we require from the cameras and minimize shadows. This level of control will give the flexibility to tune the system to get the most out of it. The LEDs are fused in such a way that a short across any point in the strip will trip an internal fuse preventing damage to the strip or any component which encounters it.</p>
                            </div>
                            <img src="./assets/mechanical11.png">
                        </div>
                    </div>
                </li>
                <!--Post 5-->
                <li>
                    <h2 class="blog-link">Controls Development</h2>
                    <h3 class="blog-date">Nov 24, 2023</h3>
                    <div class="blog-post">
                        <button type="button" class="blog-close">
                            <img src="./assets/close button.svg" alt="Close" style="width: 2em; height: auto; margin: 0;">
                        </button>
                        <div class="blog-title-box">
                            <h2 class="blog-title">Controls Development</h2>
                            <p class="blog-subtitle">Our Progress on Understanding the Controls Needed</p>
                        </div>
                        <div class="blog-content">
                            <div class="text-box">
                                <h2>A High Level Overview</h2>
                            </div>
                            <div class="text-box">
                                <p>Precision is paramount in addressing the primary control challenge of a robotic arm. This includes the pursuit of zero steady-state error and minimal overshoot. Achieving this hinges on optimizing the joint behavior governed by the motors, ensuring impeccable accuracy and swift settling times.</p>
                            </div>
                            <div class="text-box">
                                <p>The Dynamixel servos we've chosen are closed-loop motors equipped with their own onboard microcontroller. Attempting your own PID control isn't necessary as these servos lack analog control of input voltage. They already possess an effective onboard controller that manages commanded torque, positioning, velocity profiles, and response time seamlessly.</p>
                            </div>
                            <div class="text-box">
                                <p>We will begin by using the default PID controller that is on the Dynamixel servos when purchased. If we find that due to the dynamics of our system that the default PID values are not optimal, we will follow the Ziegler-Nichols method [5] for PID tuning. The order of tuning should be backwards, from end-effector to base. Starting the tuning process from the end-effector of the robotic arm and moving towards the base allows us to focus on improving task performance first, considering how errors might affect different parts of the arm, and making adjustments that suit each section's unique dynamics along the way.</p>
                            </div>
                        </div>
                    </div>
                </li>
                <!--Post 4-->
                <li>
                    <h2 class="blog-link">Software Development</h2>
                    <h3 class="blog-date">Nov 16, 2023</h3>
                    <div class="blog-post">
                        <button type="button" class="blog-close">
                            <img src="./assets/close button.svg" alt="Close" style="width: 2em; height: auto; margin: 0;">
                        </button>
                        <div class="blog-title-box">
                            <h2 class="blog-title">Software Development</h2>
                            <p class="blog-subtitle">A Reflective Log at our Software Development</p>
                        </div>
                        <div class="blog-content">
                            <div class="text-box">
                                <h2>Language Selection</h2>
                            </div>
                            <div class="text-box">
                                <p>Given the substantial software component of this project, our choice gravitated towards ROS (Robot Operating System). ROS, not a programming language but a middleware, provides a framework that supports multiple languages like C++ and Python, offering a robust ecosystem of libraries and tools for building complex robotic systems with flexibility in language choice. Accessing these resources holds immense value for our team, especially considering our limited background in robotics software. The Humble distribution of ROS2 will be used as it is the latest stable release of ROS2.</p>
                            </div>
                            <div class="text-box">
                                <p>For our project, the choice between Python and C++ within the ROS framework was a thoughtful process. While C++ offered powerful performance and precise control, Python's readability and quick prototyping won us over. We were drawn to Python's simplicity, its wealth of libraries, and how it streamlined our work on complex algorithms and hardware connections. Ultimately, Python's user-friendly nature and faster development cycles aligned better with our team's needs.</p>
                            </div>
                            <div class="text-box">
                                <h2>Architecture</h2>
                            </div>
                            <div class="text-box">
                                <p>As a starting point, we researched industry standards in robotics, focusing on arms using Dynamixel servos. Two arms that were investigated in depth were the InterbotixX series arms from Trossen Robotics and Robotis' openManipulatorX series arm. Both offer 4, 5, and 6 DOF arms with central open-source ROS repositories. </p>
                            </div>
                            <div class="text-box">
                                <p>Upon exploring the InterbotixX Arm repository (interbotix_ros_manipulators/interbotix_ros_xsarms), it was discovered that Interbotix has created an open-source ROS toolbox that features an API meant to command robotic arms powered by Dynamixel servos. Leveraging Dynamixels' ROS-compatible SDK from Robotis, this API communicates with the servo motors, enabling streamlined trajectory execution via straightforward commands. The API fetches crucial robot information from the robots URDF file and necessitates motor and mode configuration via YAML files, alongside matrices defining the robot's forward kinematics and joint screw axes. This chosen API streamlines the development process for controlling an Interbotix arm and gripper by abstracting complex ROS interactions into easy-to-use Python classes. This abstraction simplifies movement control through intuitive methods, such as setting joint positions, defining end effector poses, and executing trajectories. Additionally, it ensures safety by checking joint limits, offers predefined arm positions for convenience, and provides functions for status retrieval, like retrieving current joint commands and end effector poses. Embracing this API in our project promises a significant reduction in code complexity thanks to its ability to facilitate straightforward control and manipulation of robotic arms and grippers. Hence, we would like to leverage this API in our project.</p>
                            </div>
                            <div class="text-box">
                                <p>Following the Interbotix structure there will be 2 ROS packages from interbotix in the code: interbotix_xs_sdk, and interbotix_xs_modules which has the arm API. Furthermore, we will need a ROS package to launch the Dynamixel SDK, which we will call hawaii_control. We will need a package to load in the robots URDF (Unified Robotics Description Format) file and launch the robot state publisher and joint state publisher, which we will call hawaii_descriptions. Finally we will need a package to launch two separate instances of the aforementioned packages to accommodate for there being two robotic arms in the system. This final package will be called hawaii_teleop.</p>
                            </div>
                            <div class="text-box">
                                <p>Creating multiple packages to handle various aspects of the code instead of one monolithic package is deliberate. Employing multiple distinct ROS packages ensures a modular project structure, dividing it into more manageable units. This strategy fosters code reuse, eases maintenance, and enables targeted enhancements to specific components, all without affecting the entire project.</p>
                            </div>
                            <img src="./assets/software1.png">
                            <div class="text-box">
                                <p>As shown below, the hawaii_teleop ROS package will start communication with 2 arms and 4 cameras. Each arm will interact with separate instances of the Dynamixel control SDK. Commands will pass from our desktop through a USB converter to the student servos. On the receiving end, a teensy microcontroller on our Honolulu board will read joint states from the teacher arm encoders and broadcast them to the desktop. These joint states will be used to command the student arms via the ARM API. We'll record images, joint positions, and velocities and compile them into an HDF5 file for potential model training, though our primary goal is teleoperation and data collection.</p>
                            </div>
                            <div class="text-box">
                                <p>This modular software architecture also allows for one arm tele-operation as well as two arm tele-operation. For two arm tele-operation, the user can launch the hawaii_teleop package. Additionally, for single arm tele-operation, the user can launch the hawaii_control package, providing the name of the arm it would like to use.</p>
                            </div>
                            <div class="text-box">
                                <h2>Forward Kinematics and Screw Axes</h2>
                            </div>
                            <div class="text-box">
                                <p>An essential part of robotic control is the forward kinematics of the system. As there are elements of the system that may still be revised as we get into the assembly and commissioning phases of the project, the kinematics of the system may change. Rather than performing the complex matrix operations by hand each time, a script was created which can generate a transformation matrix (from base to end effector) and a screw axes matrix of the system from the robots URDF file. This will allow quick and iterative design cycles where the URDF file can be exported from SolidWorks, and fed into the script to generate the appropriate matrices. </p>
                            </div>
                            <div class="text-box">
                                <p>The script operates via three primary functions:<br>
                                    1.	_build_link_lists: Constructs the lists of transformations and links. It iterates backward from the specified end effector link to the base frame, gathering information about links, joints, transformations, and revolute joints and their axes. This builds essential structures used in further calculations.<br>
                                    2.	_calc_M: Computes the homogeneous transformation matrix (M) representing the end effector's pose in SE(3) space. It utilizes forward kinematics to derive this matrix based on the specified base_frame.<br>
                                    3.	_calc_Slist: Determines the list of screw axes. It calculates the screw axes using the stored axis and origin information, building a 6xN matrix where N is the number of revolute axes.<br>
                                    </p>
                            </div>
                            <div class="text-box">
                                <p>The scripts functionality was validated by utilizing a URDF file for the Viperx300 arm from Trossen Robotics. The results generated by the script were cross-referenced with the forward kinematics and screw axes matrices available in their arm's code base. The script's output perfectly matched the existing data.</p>
                            </div>
                            <div class="text-box">
                                <h2>Teacher Arm</h2>
                            </div>
                            <div class="text-box">
                                <p>The teacher arms will feature 7 encoders each. The encoders are absolute encoders, therefore we will use the values of the encoders at each joint of the teacher arms (likely with an offset) to control the positions of each joint on the student arms. </p>
                            </div>
                            <div class="text-box">
                                <p>One of the reasons that the Teensy 4.1 was chosen, is that it offers separate I2C hardware interfaces. Since we have 2 arms we will connect the encoders of the left teacher arm to one I2C port and the encoders of the right teacher arm to another. In Arduino each instance of Wire (or Wire1, Wire2, etc., depending on how many I2C ports your microcontroller supports) manages its own I2C communication, allowing for simultaneous communication with multiple I2C devices connected to different ports. This allows for concurrency without blocking as they operate on separate hardware interfaces. Therefore we will use Wire for one arm and Wire1 for the other.</p>
                            </div>
                            <div class="text-box">
                                <p>We have set an objective sampling rate of 60 readings  (of all 14 encoders) per second. We should be able to reach this goal based on the following calculations.</p>
                            </div>
                            <div class="text-box">
                                <p>We will need a transaction of 4 bytes with 11 bits each (1 start bit, 8 address bits, 1 R/W bit and 1 ACK bit) to read a sensor over I2C. This results in 44 bits. At 100kHz that is 440us. However the chip has propagation delays totaling 63.2us (t_start_prop + t_stop_prop + clk stretch time) so we get a total of 503.2us to sample 1 sensor.</p>
                            </div>
                            <div class="text-box">
                                <p>For 7 encoders (or 2x7 in parallel) it will take 3.52ms and for 14 sensors one at a time it will take 7.04ms. To sample at 30Hz for both arms in parallel it will take 105.6ms and at 60Hz, it will take 211.34ms. Therefore, we should be able to easily reach our 60Hz sampling rate objective.</p>
                            </div>
                            <div class="text-box">
                                <p>The code to read from all encoders is fairly straightforward. First, we will write a value of 0x0E to the sensor indicating that we want to read a raw angle. Then we will request the angle data from the encoder and convert it to degrees/radians. This will happen in parallel for each arms chain of encoders. An example is shown below.</p>
                            </div>
                            <img src="./assets/software2.png">
                            <div class="text-box">
                                <p>The angle data for each cycle will be grouped into packets which can be easily ingested by the desktop computer. The structure of the data will be something like this:</p>
                            </div>
                            <div class="text-box">
                                <p>{J1R, J2R, J3R, J4R, J5R, J6R, GR, J1L, J2L, J3L, J4L, J5L, J6L, GL}<br>
                                    {0.123, 0.456, 0.789, ... , 0.987}<br>
                                    {1.234, 1.567, 1.890, ... , 1.987}
                                    </p>
                            </div>
                            <div class="text-box">
                                <p>The first packet will identify which joint and which arm (right or left) each value in the packet corresponds to. The following packets will hold the angular data received from the encoders. We will not wait for an acknowledgement from the desktop. Firstly, we will be able to see whether or not the student arms are matching the teacher arms movements. Secondly, since the encoders are absolute, losing occasional data packets doesn't significantly impact our application. </p>
                            </div>
                            <div class="text-box">
                                <h2>Proof of Concept</h2>
                            </div>
                            <div class="text-box">
                                <p>It was essential to verify that the Interbotix arm API can be used to control our robot design. We started by investigating how the Interbotix arm API is used by Trossen Robotics themselves. As discussed earlier, the API requires access to the robots description (which can be found via the URDF), the robots forward kinematics and screw axes matrices, and yaml files identifying which dynamixel motor corresponds to each joint, and defining the operating modes of the motors. A challenge that we came across is that the gripper design Trossen Robotics uses is quite different from the gripper design of our system. In order to address this, we located the code pertaining to the gripper in the API and made modifications in order to support our gripper configuration.</p>
                            </div>
                            <div class="text-box">
                                <p>As shown below, our robot can be visualized in Rviz2 simulation. Furthermore, we have been able to fully control the robot with the Interbotix API with the aforementioned modifications.</p>
                            </div>
                            <img src="./assets/rviz.gif">
                            <div class="text-box">
                                <p>We also needed to validate that the Interbotix API could be used to control a Dynamixel servo. We created a custom URDF file which featured a single joint of a robot. Additionally, a YAML file was created to point the Dynamixel SDK to a servo with a matching ID over the proper serial port. We were successfully able to command the servo to a variety of positions via the arm API. In the image below, you can see that the position of the joint in simulation matches the position of the servo in real life.</p>
                            </div>
                            <img src="./assets/software3.png">
                            <div class="text-box">
                                <p>Stay tuned for updates as we get more hardware together and start integration!</p>
                            </div>
                        </div>
                    </div>
                </li>
                <!--Post 3-->
                <li>
                    <h2 class="blog-link">Electrical Development</h2>
                    <h3 class="blog-date">Nov 10, 2023</h3>
                    <div class="blog-post">
                        <button type="button" class="blog-close">
                            <img src="./assets/close button.svg" alt="Close" style="width: 2em; height: auto; margin: 0;">
                        </button>
                        <div class="blog-title-box">
                            <h2 class="blog-title">Controls Development</h2>
                            <p class="blog-subtitle">Our Progress on Understanding the Controls Needed</p>
                        </div>
                        <div class="blog-content">
                            <div class="text-box">
                                <h2>High Lever Overview</h2>
                            </div>
                            <div class="text-box">
                                <p>Precision is paramount in addressing the primary control challenge of a robotic arm. This includes the pursuit of zero steady-state error and minimal overshoot. Achieving this hinges on optimizing the joint behavior governed by the motors, ensuring impeccable accuracy and swift settling times.</p>
                            </div>
                            <div class="text-box">
                                <p>The Dynamixel servos we've chosen are closed-loop motors equipped with their own onboard microcontroller. Attempting your own PID control isn't necessary as these servos lack analog control of input voltage. They already possess an effective onboard controller that manages commanded torque, positioning, velocity profiles, and response time seamlessly.</p>
                            </div>
                            <div class="text-box">
                                <p>We will begin by using the default PID controller that is on the Dynamixel servos when purchased. If we find that due to the dynamics of our system that the default PID values are not optimal, we will follow the Ziegler-Nichols method [5] for PID tuning. The order of tuning should be backwards, from end-effector to base. Starting the tuning process from the end-effector of the robotic arm and moving towards the base allows us to focus on improving task performance first, considering how errors might affect different parts of the arm, and making adjustments that suit each section's unique dynamics along the way.</p>
                            </div>
                        </div>
                    </div>
                </li>
                <!--Post 2-->
                <li>
                    <h2 class="blog-link">PDP</h2>
                    <h3 class="blog-date">Oct 17, 2023</h3>
                    <div class="blog-post">
                        <button type="button" class="blog-close">
                            <img src="./assets/close button.svg" alt="Close" style="width: 2em; height: auto; margin: 0;">
                        </button>
                        <div class="blog-title-box">
                            <h2 class="blog-title">PDP Slides</h2>
                            <p class="blog-subtitle">Preliminary Design Presentation</p>
                        </div>
                        <div class="blog-content">
                            <img style="margin-top: 16px;" src="./assets/PDP/PDP_Slide (1).PNG">
                            <img src="./assets/PDP/PDP_Slide (2).PNG">
                            <img src="./assets/PDP/PDP_Slide (3).PNG">
                            <img src="./assets/PDP/PDP_Slide (4).PNG">
                            <img src="./assets/PDP/PDP_Slide (5).PNG">
                            <img src="./assets/PDP/PDP_Slide (6).PNG">
                            <img src="./assets/PDP/PDP_Slide (7).PNG">
                            <img src="./assets/PDP/PDP_Slide (8).PNG">
                            <img src="./assets/PDP/PDP_Slide (9).PNG">
                            <img src="./assets/PDP/PDP_Slide (10).PNG">
                            <img src="./assets/PDP/PDP_Slide (11).PNG">
                            <img src="./assets/PDP/PDP_Slide (12).PNG">
                            <img src="./assets/PDP/PDP_Slide (13).PNG">
                        </div>
                    </div>
                </li>
                <!--Post 1-->
                <li>
                    <h2 class="blog-link">A Problem Worth Solving</h2>
                    <h3 class="blog-date">Sep 18, 2023</h3>
                    <div class="blog-post">
                        <button type="button" class="blog-close">
                            <img src="./assets/close button.svg" alt="Close" style="width: 2em; height: auto; margin: 0;">
                        </button>
                        <div class="blog-title-box">
                            <h2 class="blog-title">A Problem Worth Solving</h2>
                            <p class="blog-subtitle">Selecting a problem worth solving, acheivable, and that excites us all</p>
                        </div>
                        <div class="blog-content">
                            <div class="text-box">
                                <h2>Pre-Idea</h2>
                            </div>
                            <div class="text-box">
                                <p>Our journey began in the summer, calling every week to try to find a problem worth solving, that we're all passionate about, and that could be completed in the 8 months given. Our team, a diverse group of thinkers and tinkerers, united with a shared vision: to push the boundaries of robotics and AI research. We set out to explore ideas that could not only redefine technological possibilities but also offer tangible solutions to real-world challenges. Two of these were finding use cases for robotics, something we were all passionate about, and one of the problems had to do with making cooking easier - something we all wanted to solve.</p>
                            </div>
                            <div class="text-box">
                                <h2>Early Days: A Mobile Robot for Research Labs</h2>
                            </div>
                            <div class="text-box">
                                <p>Our initial concept was straightforward yet bold. We envisioned a "general purpose mobile manipulator" tailored for research labs. The core philosophy was to democratize robotics research by creating affordable, open-source platforms. This initiative aimed to accelerate embodied AI research by minimizing costs to maximize adoption, fostering a network effect through crowdsourcing data and papers.</p>
                            </div>
                            <div class="text-box">
                                <p>The long term vision was to build a ubiquitous mobile manipulation platform, much like what the Franka Panda did for robot arms, but with more versatile applications. Our moonshot milestone was even more ambitious: leveraging a vast dataset to incrementally solve the complexities of general robotics, progressively venturing into more challenging and unstructured environments as AI technology advanced. Essentially riding the wave of computational intelligence with a robotics platform that was limited by intelligence, not hardware.</p>
                            </div>
                            <div class="text-box">
                                <p>We acknowledged the dichotomy in the current robotics landscape: on one end, research labs struggled with expensive, clunky hardware; on the other, companies produced excellent hardware with no solid approach to embodied AI. Our solution intended to bridge this gap, providing an open-source platform rich in diverse, unlabelled data such as RGBD main camera footage, robot poses, and joint torques.</p>
                            </div>
                            <img src="./assets/pareto.png">
                            <div class="text-box">
                                <h2>A Shift in Focus: Smart Kitchen Assistant</h2>
                            </div>
                            <div class="text-box">
                                <p>Midway through our brainstorming sessions, our attention shifted towards a more domestic application: a smart camera system for kitchen stoves, named "Sous." The problem was clear - cooking requires constant monitoring, a skill not everyone possesses. For those that suck at cooking, or can’t help being distracted, having something to watch the stove top for you is very valuable. Sous would ideally monitor cooking parameters like time, temperature, and state of food using advanced computer vision, easing the cooking process.</p>
                            </div>
                            <div class="text-box">
                                <p>Our short term goal was to launch a hardware product that could oversee stovetop cooking, potentially evolving to control a cooking robot. Sous was more than a gadget; it was a step towards an effortless, high-quality culinary experience, leveraging data to enhance its capabilities - and following the same theme as before of riding the intelligence wave.</p>
                            </div>
                            <img src="./assets/sous.png">
                            <div class="text-box">
                                <h2>Final Frontier: Bimanual Teleoperation System</h2>
                            </div>
                            <div class="text-box">
                                <p>As our ideas matured, we honed in on a challenge that resonated with our core competencies and aspirations: developing a teleoperation system for bimanual robots. This endeavor addresses the critical need for flexibility and precision in small-scale manufacturing environments, where manual tasks of high complexity and variability are prevalent.</p>
                            </div>
                            <div class="text-box">
                                <p>To be clear, we sort’ve selected a solution ahead of a problem, but given that this field has only recently begin to grow we think this is good. So we needed to find a problem that we could solve by enabling better robot teleoperation - we settled on small scale manufacturing. Small scale manufacturing is low risk, slower, and less intense than full mass manufacturing, but could still use the advantages of modern robotics. This system would leverage cutting-edge diffusion-based imitation learning to perform precise, dexterous tasks, thus aiding small manufacturing runs and bridging the gap between prototype development and mass production.</p>
                            </div>
                            <div class="text-box">
                                <p>The design specifications for this project are challenging but clearly defined. We are committed to overcoming the traditional barriers of inflexibility and high cost in manufacturing automation by using newly developed machine learning architectures.</p>
                            </div>
                            <div class="text-box">
                                <p>Our journey from a mobile robot platform for research labs to a teleoperated bimanual system has been nothing short of transformative. Each pivot and iteration brought us closer to a problem worth solving, and we truly hope we’ve landed on it.</p>
                            </div>
                            <img src="./assets/hawaii_dalle.png">
                        </div>
                    </div>
                </li>
            </ul>
        </div>
    </main>
    <footer>
        <p>hawaii robotics</p>
        <p>made with &hearts; in canada</p>
    </footer>
    <script>
        const triggers = document.getElementsByClassName("blog-link");
        const modals = document.getElementsByClassName("blog-post");
        const closeButtons = document.getElementsByClassName("blog-close");

        // Function to toggle the visibility of the modal
        const toggleModal = (index) => {
            modals[index].classList.toggle("show-modal");
        };

        // Function to handle clicking outside the modal
        const clickOutsideModal = (event) => {
            for (let i = 0; i < modals.length; i++) {
                if (modals[i].classList.contains("show-modal") && !modals[i].contains(event.target)) {
                    toggleModal(i);
                }
            }
        };

        // Add event listeners to triggers and close buttons
        for (let i = 0; i < triggers.length; i++) {
            // Toggle modal on trigger click
            triggers[i].addEventListener("click", (event) => {
                event.stopPropagation(); // Prevent click from bubbling to the document
                toggleModal(i);
            });

            // Toggle modal on close button click
            closeButtons[i].addEventListener("click", (event) => {
                event.stopPropagation(); // Prevent click from bubbling to the document
                toggleModal(i);
            });
        }

        // Event listener for clicks outside of modals
        document.addEventListener("click", clickOutsideModal);









    </script>
</body>
</html>